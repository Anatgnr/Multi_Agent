#### For OLLAMA server use the following configuration:

# CAUTION: Make sure to have OLLAMA server running and the models downloaded locally.
    # You can download models using the command: ollama pull <model_name>
    # Feel free to change the model if you have a different preferences.

architect:
  model: llama3.2:latest
backend:
  model: codellama:7b
windowsappmaker:
  model: mistral:latest
docwriter:
  model: phi4:14b
